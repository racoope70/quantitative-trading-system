{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/racoope70/quant-trading-model-zoo/blob/main/deployable_multi_stock_ppo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFYht8BLBDv5"
      },
      "outputs": [],
      "source": [
        "!pip install \"shimmy>=2.0.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xMJutMCD_uu"
      },
      "outputs": [],
      "source": [
        "!pip -q install yfinance pywavelets transformers --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILVF8lQ7ibyY"
      },
      "outputs": [],
      "source": [
        "!apt-get remove --purge -y cuda* libcuda* nvidia* || echo \"No conflicting CUDA packages\"\n",
        "!apt-get autoremove -y\n",
        "!apt-get clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ss1DpUiHinrL"
      },
      "outputs": [],
      "source": [
        "!apt-get update -qq && apt-get install -y \\\n",
        "    libcusolver11 libcusparse11 libcurand10 libcufft10 libnppig10 libnppc10 libnppial10 \\\n",
        "    cuda-toolkit-12-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-3wUYhMjK-s"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y protobuf\n",
        "!pip install protobuf==3.20.3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeXuomvZjPD6"
      },
      "outputs": [],
      "source": [
        "!pip install --extra-index-url=https://pypi.nvidia.com \\\n",
        "    cuml-cu12==25.2.0 cudf-cu12==25.2.0 cupy-cuda12x \\\n",
        "    dask-cuda==25.2.0 dask-cudf-cu12==25.2.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_tMI99TjTni"
      },
      "outputs": [],
      "source": [
        "!pip install numba==0.60.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kHVvDWyvwdt"
      },
      "outputs": [],
      "source": [
        "!pip install \"stable-baselines3[extra]>=2.0.0\" \"gymnasium>=0.29\" \"shimmy>=2.0.0\" \\\n",
        "  gym-anytrading yfinance pandas numpy scikit-learn xgboost joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cFCJkVhjYcj"
      },
      "outputs": [],
      "source": [
        "#!pip install stable-baselines3[extra] gymnasium gym-anytrading yfinance xgboost joblib\n",
        "#!pip install matplotlib scikit-learn pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDqANXjRjbY8"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.18.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTQMOopCjd_Y"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xlm1PrOjgys"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"TensorFlow GPU memory growth enabled\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"TensorFlow GPU memory config failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gUz7N8ajjqs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_HOME'] = '/usr/local/cuda-12.4'\n",
        "os.environ['PATH'] += ':/usr/local/cuda-12.4/bin'\n",
        "os.environ['LD_LIBRARY_PATH'] += ':/usr/local/cuda-12.4/lib64'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxTo_GOSemtU"
      },
      "outputs": [],
      "source": [
        "# Authenticate with Hugging Face hub (optional)\n",
        "# This allows for better access and avoids rate limits when downloading public models/datasets\n",
        "\n",
        "# Authenticate with Hugging Face Hub\n",
        "# Notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9AoPa7K4FEK"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os, gc, time, json, pywt, logging, warnings\n",
        "from datetime import datetime, timedelta\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "\n",
        "# Feature Toggles / Config\n",
        "USE_SENTIMENT = False\n",
        "USE_REGIME    = True\n",
        "TEST_MODE     = False\n",
        "\n",
        "INTERVAL     = \"1h\"\n",
        "PERIOD_DAYS  = 720\n",
        "FWD_HORIZON  = 10\n",
        "UP_THR       = 0.02\n",
        "DN_THR       = -0.02\n",
        "\n",
        "# Train/Validation split (by time)\n",
        "VAL_FRACTION = 0.20\n",
        "\n",
        "# Output paths\n",
        "LOCAL_OUT   = \"multi_stock_feature_engineered_dataset.csv\"\n",
        "LOCAL_TRAIN = \"train.csv\"\n",
        "LOCAL_VAL   = \"val.csv\"\n",
        "PARQ_FULL   = \"features_full.parquet\"\n",
        "PARQ_TRAIN  = \"train.parquet\"\n",
        "PARQ_VAL    = \"val.parquet\"\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DRIVE_BASE = \"/content/drive/MyDrive\"\n",
        "except Exception:\n",
        "    DRIVE_BASE = os.getcwd()\n",
        "\n",
        "DRIVE_DIR = os.path.join(DRIVE_BASE, \"trading_data\")\n",
        "os.makedirs(DRIVE_DIR, exist_ok=True)\n",
        "\n",
        "# Logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "logging.info(f\"yfinance version: {getattr(yf, '__version__', 'unknown')}\")\n",
        "logging.info(f\"pandas version: {pd.__version__}\")\n",
        "\n",
        "ticker_list = [\n",
        "    'AAPL','TSLA','MSFT','GOOGL','AMZN','NVDA','META','BRK-B','JPM','JNJ',\n",
        "    'XOM','V','PG','UNH','MA','HD','LLY','MRK','PEP','KO',\n",
        "    'BAC','ABBV','AVGO','PFE','COST','CSCO','TMO','ABT','ACN','WMT',\n",
        "    'MCD','ADBE','DHR','CRM','NKE','INTC','QCOM','NEE','AMD','TXN',\n",
        "    'AMGN','UPS','LIN','PM','UNP','BMY','LOW','RTX','CVX','IBM',\n",
        "    'GE','SBUX','ORCL'\n",
        "]\n",
        "SYMBOLS = ['AAPL', 'NVDA', 'MSFT'] if TEST_MODE else ticker_list\n",
        "\n",
        "if USE_SENTIMENT:\n",
        "    try:\n",
        "        import torch\n",
        "        from transformers import pipeline\n",
        "        device_id = 0 if torch.cuda.is_available() else -1\n",
        "        sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"ProsusAI/finbert\", device=device_id)\n",
        "        logging.info(\"FinBERT sentiment enabled.\")\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Could not init FinBERT; disabling sentiment. Err: {e}\")\n",
        "        USE_SENTIMENT = False\n",
        "        sentiment_pipeline = None\n",
        "else:\n",
        "    sentiment_pipeline = None\n",
        "\n",
        "# Helpers\n",
        "def _force_datetime_column(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Ensure a tz-naive 'Datetime' column exists even if the index had no name.\n",
        "    Deduplicate/sort by Datetime.\n",
        "    \"\"\"\n",
        "    if isinstance(df.index, pd.DatetimeIndex):\n",
        "        try:\n",
        "            if df.index.tz is not None:\n",
        "                df.index = df.index.tz_convert(None)\n",
        "        except Exception:\n",
        "            try:\n",
        "                df.index = df.index.tz_localize(None)\n",
        "            except Exception:\n",
        "                pass\n",
        "        df.index.name = 'Datetime'\n",
        "        df = df.reset_index()\n",
        "    else:\n",
        "        df = df.reset_index()\n",
        "        first = df.columns[0]\n",
        "        if np.issubdtype(df[first].dtype, np.datetime64):\n",
        "            df = df.rename(columns={first: 'Datetime'})\n",
        "        elif 'Date' in df.columns:\n",
        "            df['Datetime'] = pd.to_datetime(df['Date'])\n",
        "        elif 'Datetime' not in df.columns:\n",
        "            try:\n",
        "                df['Datetime'] = pd.to_datetime(df[first], errors='coerce')\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    if 'Datetime' not in df.columns:\n",
        "        raise KeyError(\"Failed to construct 'Datetime' column from yfinance output.\")\n",
        "\n",
        "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
        "    df = (df.drop_duplicates(subset=['Datetime'])\n",
        "            .sort_values('Datetime')\n",
        "            .reset_index(drop=True))\n",
        "    return df\n",
        "\n",
        "def _normalize_ohlcv(df_in: pd.DataFrame, ticker: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Normalize column names from yfinance:\n",
        "    - Flatten MultiIndex\n",
        "    - Strip ticker at prefix/suffix (e.g., 'AAPL Open' or 'Open AAPL')\n",
        "    - Map variants to canonical: Open, High, Low, Close, Adj Close, Volume\n",
        "    \"\"\"\n",
        "    import re\n",
        "    df = df_in.copy()\n",
        "\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        flat = []\n",
        "        for col in df.columns:\n",
        "            parts = [str(p) for p in col if p is not None and str(p) != \"\"]\n",
        "            flat.append(\" \".join(parts))\n",
        "        df.columns = flat\n",
        "\n",
        "    df.columns = [re.sub(r\"\\s+\", \" \", str(c)).strip() for c in df.columns]\n",
        "\n",
        "    tkr = ticker.upper().replace(\"-\", \"[- ]?\")\n",
        "    cleaned = {}\n",
        "    for c in df.columns:\n",
        "        c_up = c.upper()\n",
        "        c2 = re.sub(rf\"^(?:{tkr})[\\s/_-]+\", \"\", c_up)   # Leading ticker\n",
        "        c2 = re.sub(rf\"[\\s/_-]+(?:{tkr})$\", \"\", c2)     # Trailing ticker\n",
        "        c2 = c2.title()\n",
        "        cleaned[c] = c2\n",
        "    if any(cleaned[c] != c for c in df.columns):\n",
        "        df = df.rename(columns=cleaned)\n",
        "\n",
        "    cols_ci = {c.lower(): c for c in df.columns}\n",
        "    wants = {\n",
        "        \"Open\":      [\"open\"],\n",
        "        \"High\":      [\"high\"],\n",
        "        \"Low\":       [\"low\"],\n",
        "        \"Close\":     [\"close\", \"close*\", \"last\"],\n",
        "        \"Adj Close\": [\"adj close\", \"adj_close\", \"adjclose\", \"adjusted close\"],\n",
        "        \"Volume\":    [\"volume\", \"vol\"]\n",
        "    }\n",
        "    rename_map = {}\n",
        "    for desired, alts in wants.items():\n",
        "        if desired.lower() in cols_ci:\n",
        "            rename_map[cols_ci[desired.lower()]] = desired\n",
        "            continue\n",
        "        for a in alts:\n",
        "            if a in cols_ci:\n",
        "                rename_map[cols_ci[a]] = desired\n",
        "                break\n",
        "    if rename_map:\n",
        "        df = df.rename(columns=rename_map)\n",
        "\n",
        "    return df\n",
        "\n",
        "def download_stock_data(ticker, interval=\"1h\", period_days=720, max_retries=5, sleep_base=3):\n",
        "    \"\"\"\n",
        "    Robust yfinance intraday downloader with schema normalization and history() fallback.\n",
        "    Ensures: Open, High, Low, Close, Volume (Adj Close synthesized if missing) + Datetime + Symbol.\n",
        "    \"\"\"\n",
        "    period_str = f\"{int(period_days)}d\"\n",
        "\n",
        "    def _postprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
        "        df = _normalize_ohlcv(df, ticker)\n",
        "        df = _force_datetime_column(df)\n",
        "        needed = {'Open', 'High', 'Low', 'Close', 'Volume'}\n",
        "        missing = needed - set(df.columns)\n",
        "        if missing:\n",
        "            logging.debug(f\"[{ticker}] columns received: {list(df.columns)}\")\n",
        "            raise ValueError(f\"Missing OHLCV columns after normalize: {missing}\")\n",
        "        if 'Adj Close' not in df.columns:\n",
        "            df['Adj Close'] = df['Close']\n",
        "        return df\n",
        "\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            logging.info(f\" [{ticker}] Attempt {attempt}: download(period={period_str}, interval={interval})\")\n",
        "            df = yf.download(\n",
        "                tickers=ticker,\n",
        "                period=period_str,\n",
        "                interval=interval,\n",
        "                progress=False,\n",
        "                auto_adjust=False,\n",
        "                group_by='column',\n",
        "                threads=False,\n",
        "                prepost=False,\n",
        "                repair=True\n",
        "            )\n",
        "            if df is None or df.empty:\n",
        "                raise ValueError(\"Empty data from download()\")\n",
        "            if attempt == 1:\n",
        "                logging.debug(f\"[{ticker}] raw columns (download): {list(df.columns)}\")\n",
        "\n",
        "            df = _postprocess(df)\n",
        "            df['Symbol'] = ticker\n",
        "            logging.info(f\" [{ticker}] rows: {len(df)} from {df['Datetime'].min()} to {df['Datetime'].max()}\")\n",
        "            return df\n",
        "\n",
        "        except Exception as e1:\n",
        "            logging.warning(f\" [{ticker}] download normalize error: {e1} | trying history() fallback\")\n",
        "            try:\n",
        "                hist = yf.Ticker(ticker).history(\n",
        "                    period=period_str,\n",
        "                    interval=interval,\n",
        "                    auto_adjust=False,\n",
        "                    actions=False\n",
        "                )\n",
        "                if hist is None or hist.empty:\n",
        "                    raise ValueError(\"Empty data from history()\")\n",
        "                if attempt == 1:\n",
        "                    logging.debug(f\"[{ticker}] raw columns (history): {list(hist.columns)}\")\n",
        "\n",
        "                df = _postprocess(hist)\n",
        "                df['Symbol'] = ticker\n",
        "                logging.info(f\" [{ticker}] (fallback) rows: {len(df)} from {df['Datetime'].min()} to {df['Datetime'].max()}\")\n",
        "                return df\n",
        "\n",
        "            except Exception as e2:\n",
        "                wait = sleep_base * attempt\n",
        "                logging.warning(f\" [{ticker}] history() error: {e2} | retrying in {wait}s\")\n",
        "                time.sleep(wait)\n",
        "\n",
        "    logging.error(f\" [{ticker}] Failed to download after {max_retries} attempts.\")\n",
        "    return None\n",
        "\n",
        "def denoise_wavelet(series, wavelet='db1', level=2):\n",
        "    s = pd.Series(series).astype(float).ffill().bfill().to_numpy()\n",
        "    try:\n",
        "        coeffs = pywt.wavedec(s, wavelet, mode='symmetric', level=level)\n",
        "        for i in range(1, len(coeffs)):\n",
        "            coeffs[i] = np.zeros_like(coeffs[i])\n",
        "        rec = pywt.waverec(coeffs, wavelet, mode='symmetric')\n",
        "        return pd.Series(rec[:len(s)], index=series.index)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Wavelet denoising failed ({e}); using raw Close.\")\n",
        "        return pd.Series(s, index=series.index)\n",
        "\n",
        "def score_sentiment(texts):\n",
        "    if not USE_SENTIMENT or sentiment_pipeline is None:\n",
        "        return [0.0] * len(texts)\n",
        "    try:\n",
        "        outputs = sentiment_pipeline(texts, truncation=True, max_length=256, batch_size=32)\n",
        "        scores = []\n",
        "        for r in outputs:\n",
        "            label = r['label'].lower()\n",
        "            if label == 'positive':\n",
        "                scores.append(+float(r['score']))\n",
        "            elif label == 'negative':\n",
        "                scores.append(-float(r['score']))\n",
        "            else:\n",
        "                scores.append(0.0)\n",
        "        return scores\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Sentiment scoring error: {e}\")\n",
        "        return [0.0] * len(texts)\n",
        "\n",
        "def add_regime(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df['Vol20'] = df['Close'].pct_change().rolling(20).std()\n",
        "    df['Ret20'] = df['Close'].pct_change(20)\n",
        "    vol_hi   = (df['Vol20'] > df['Vol20'].median()).astype(int)\n",
        "    trend_hi = (df['Ret20'].abs() > df['Ret20'].abs().median()).astype(int)\n",
        "    df['Regime4'] = vol_hi * 2 + trend_hi\n",
        "    return df\n",
        "\n",
        "def compute_enhanced_features(df):\n",
        "    df = df.copy()\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df.columns = df.columns.get_level_values(0)\n",
        "    df = df.loc[:, ~df.columns.duplicated()]\n",
        "\n",
        "    # Technicals\n",
        "    df['SMA_20'] = df['Close'].rolling(20).mean()\n",
        "    df['STD_20'] = df['Close'].rolling(20).std()\n",
        "    df['Upper_Band'] = df['SMA_20'] + 2 * df['STD_20']\n",
        "    df['Lower_Band'] = df['SMA_20'] - 2 * df['STD_20']\n",
        "\n",
        "    df['Lowest_Low']   = df['Low'].rolling(14).min()\n",
        "    df['Highest_High'] = df['High'].rolling(14).max()\n",
        "    denom = (df['Highest_High'] - df['Lowest_Low']).replace(0, np.nan)\n",
        "    df['Stoch'] = ((df['Close'] - df['Lowest_Low']) / denom) * 100\n",
        "\n",
        "    df['ROC'] = df['Close'].pct_change(10)\n",
        "    df['OBV'] = (np.sign(df['Close'].diff()).fillna(0) * df['Volume'].fillna(0)).cumsum()\n",
        "\n",
        "    tp = (df['High'] + df['Low'] + df['Close']) / 3\n",
        "    sma_tp = tp.rolling(20).mean()\n",
        "    md = (tp - sma_tp).abs().rolling(20).mean()\n",
        "    df['CCI'] = (tp - sma_tp) / (0.015 * md)\n",
        "\n",
        "    df['EMA_10'] = df['Close'].ewm(span=10, adjust=False).mean()\n",
        "    df['EMA_50'] = df['Close'].ewm(span=50, adjust=False).mean()\n",
        "    ema12 = df['Close'].ewm(span=12, adjust=False).mean()\n",
        "    ema26 = df['Close'].ewm(span=26, adjust=False).mean()\n",
        "    df['MACD_Line']   = ema12 - ema26\n",
        "    df['MACD_Signal'] = df['MACD_Line'].ewm(span=9, adjust=False).mean()\n",
        "\n",
        "    delta = df['Close'].diff()\n",
        "    gain  = delta.clip(lower=0).rolling(14).mean()\n",
        "    loss  = (-delta.clip(upper=0)).rolling(14).mean()\n",
        "    rs = gain / (loss.replace(0, np.nan))\n",
        "    df['RSI'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "    tr = pd.concat([\n",
        "        (df['High'] - df['Low']),\n",
        "        (df['High'] - df['Close'].shift()).abs(),\n",
        "        (df['Low'] - df['Close'].shift()).abs()\n",
        "    ], axis=1).max(axis=1)\n",
        "    df['ATR'] = tr.rolling(14).mean()\n",
        "\n",
        "    df['Volatility'] = df['Close'].pct_change().rolling(20).std()\n",
        "\n",
        "    df['Denoised_Close'] = denoise_wavelet(df['Close'].ffill())\n",
        "\n",
        "    if USE_REGIME:\n",
        "        df = add_regime(df)\n",
        "\n",
        "    if USE_SENTIMENT and len(df):\n",
        "        headline = f\"{df['Symbol'].iloc[0]} is expected to perform well in the market.\"\n",
        "        try:\n",
        "            score = score_sentiment([headline])[0]\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Sentiment scoring failed for {df['Symbol'].iloc[0]}: {e}\")\n",
        "            score = 0.0\n",
        "        df['SentimentScore'] = float(score)\n",
        "    else:\n",
        "        df['SentimentScore'] = 0.0\n",
        "\n",
        "    df['Delta'] = df['Close'].pct_change(1).fillna(0)\n",
        "    df['Gamma'] = df['Delta'].diff().fillna(0)\n",
        "\n",
        "    df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "    cols = [c for c in df.columns if c not in ['Symbol']] + ['Symbol']\n",
        "    return df[cols]\n",
        "\n",
        "# Main Loop\n",
        "all_dfs = []\n",
        "for i, ticker in enumerate(SYMBOLS, 1):\n",
        "    logging.info(f\"[{i}/{len(SYMBOLS)}] Processing {ticker}\")\n",
        "    raw = download_stock_data(\n",
        "        ticker, interval=INTERVAL, period_days=PERIOD_DAYS, max_retries=5, sleep_base=3\n",
        "    )\n",
        "    if raw is None or raw.empty:\n",
        "        logging.warning(f\"No data for {ticker}\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        features = compute_enhanced_features(raw)\n",
        "        if features is not None and not features.empty:\n",
        "            logging.info(f\" [{ticker}] feature rows: {len(features)}\")\n",
        "            all_dfs.append(features)\n",
        "        else:\n",
        "            logging.warning(f\"Feature set empty for {ticker}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Feature engineering failed for {ticker}: {e}\")\n",
        "    finally:\n",
        "        del raw\n",
        "        try:\n",
        "            del features\n",
        "        except NameError:\n",
        "            pass\n",
        "        gc.collect()\n",
        "        time.sleep(0.5)\n",
        "\n",
        "if all_dfs:\n",
        "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
        "    logging.info(f\" Combined dataset shape: {final_df.shape}\")\n",
        "\n",
        "    if 'Repaired?' in final_df.columns:\n",
        "        final_df = final_df.drop(columns=['Repaired?'])\n",
        "    final_df['Datetime'] = pd.to_datetime(final_df['Datetime'], utc=True).dt.tz_convert('America/New_York')\n",
        "    dt = final_df['Datetime']\n",
        "    rth_mask = (\n",
        "        (dt.dt.weekday < 5) &\n",
        "        (dt.dt.time >= pd.to_datetime(\"09:30\").time()) &\n",
        "        (dt.dt.time <  pd.to_datetime(\"16:00\").time())\n",
        "    )\n",
        "    final_df = final_df[rth_mask].reset_index(drop=True)\n",
        "\n",
        "    def relabel(df):\n",
        "        df = df.copy()\n",
        "        df['Return'] = (df['Close'].shift(-FWD_HORIZON) - df['Close']) / df['Close']\n",
        "        df['Target'] = np.select(\n",
        "            [df['Return'] > UP_THR, df['Return'] < DN_THR],\n",
        "            [1, -1],\n",
        "            default=0\n",
        "        )\n",
        "        return df\n",
        "\n",
        "    final_df = relabel(final_df)\n",
        "    final_df = final_df.sort_values(['Symbol', 'Datetime']).reset_index(drop=True)\n",
        "    final_df['__i'] = final_df.groupby('Symbol').cumcount()\n",
        "    final_df['__n'] = final_df.groupby('Symbol')['__i'].transform('max') + 1\n",
        "    final_df = final_df[ final_df['__i'] < final_df['__n'] - FWD_HORIZON ].copy()\n",
        "    final_df = final_df.drop(columns=['__i','__n']).reset_index(drop=True)\n",
        "    feature_cols = [c for c in final_df.columns if c not in ['Target','Return','Symbol','Datetime']]\n",
        "    final_df = final_df.dropna(subset=feature_cols).reset_index(drop=True)\n",
        "    ORDER_LAST = ['Target', 'Return', 'Symbol']\n",
        "    cols = [c for c in final_df.columns if c not in ORDER_LAST] + ORDER_LAST\n",
        "    final_df = final_df[cols]\n",
        "\n",
        "    # Sanity summary\n",
        "    def summarize(df):\n",
        "        print(\" Combined dataset shape:\", df.shape)\n",
        "        print(\" Range:\", df['Datetime'].min(), \"-\", df['Datetime'].max())\n",
        "        print(\" Per-ticker counts:\")\n",
        "        print(df['Symbol'].value_counts().to_string())\n",
        "        na_cols = df.columns[df.isna().any()]\n",
        "        if len(na_cols):\n",
        "            print(\"\\n Columns with NaNs:\")\n",
        "            print(df[na_cols].isna().sum().sort_values(ascending=False).to_string())\n",
        "        else:\n",
        "            print(\"\\n No NaNs detected.\")\n",
        "        print(\"\\n Label counts:\")\n",
        "        print(df['Target'].value_counts().sort_index().to_string())\n",
        "        print(\"\\n Label ratios (%):\")\n",
        "        print((df['Target'].value_counts(normalize=True)*100).round(2).to_string())\n",
        "\n",
        "    summarize(final_df)\n",
        "\n",
        "    # Time-based Train/Validation split (80/20 by time)\n",
        "    final_df = final_df.sort_values('Datetime').reset_index(drop=True)\n",
        "    cutoff_idx = int((1.0 - VAL_FRACTION) * len(final_df))\n",
        "    cutoff_time = final_df.loc[cutoff_idx, 'Datetime']\n",
        "    train_df = final_df[ final_df['Datetime'] <  cutoff_time ].reset_index(drop=True)\n",
        "    val_df   = final_df[ final_df['Datetime'] >= cutoff_time ].reset_index(drop=True)\n",
        "\n",
        "    print(f\"\\n Time split cutoff @ {cutoff_time}\")\n",
        "    print(f\"Train: {train_df.shape},  Val: {val_df.shape}\")\n",
        "\n",
        "    # Save (CSV)\n",
        "    final_df.to_csv(LOCAL_OUT, index=False)\n",
        "    train_df.to_csv(LOCAL_TRAIN, index=False)\n",
        "    val_df.to_csv(LOCAL_VAL, index=False)\n",
        "    logging.info(f\"Saved CSVs: {LOCAL_OUT}, {LOCAL_TRAIN}, {LOCAL_VAL}\")\n",
        "\n",
        "    # Save (Parquet)\n",
        "    final_df.to_parquet(PARQ_FULL, index=False)\n",
        "    train_df.to_parquet(PARQ_TRAIN, index=False)\n",
        "    val_df.to_parquet(PARQ_VAL, index=False)\n",
        "    logging.info(f\"Saved Parquets: {PARQ_FULL}, {PARQ_TRAIN}, {PARQ_VAL}\")\n",
        "\n",
        "    # Also save to Drive\n",
        "    final_path = os.path.join(DRIVE_DIR, \"multi_stock_feature_engineered_dataset.csv\")\n",
        "    train_path = os.path.join(DRIVE_DIR, \"train.csv\")\n",
        "    val_path   = os.path.join(DRIVE_DIR, \"val.csv\")\n",
        "    final_df.to_csv(final_path, index=False)\n",
        "    train_df.to_csv(train_path, index=False)\n",
        "    val_df.to_csv(val_path, index=False)\n",
        "    logging.info(f\"Saved to Google Drive:\\n- {final_path}\\n- {train_path}\\n- {val_path}\")\n",
        "\n",
        "    # Cleanup\n",
        "    del all_dfs, final_df, train_df, val_df\n",
        "    gc.collect()\n",
        "else:\n",
        "    logging.warning(\"No usable data found for any ticker.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47KOlGdVbGMl"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"multi_stock_feature_engineered_dataset.csv\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9FCqXbnzHQU"
      },
      "outputs": [],
      "source": [
        "# PPO Walkforward with Runtime Timing, Full Model Saving, and Logging\n",
        "import os, gc, time, json, torch, logging\n",
        "import pandas as pd, numpy as np, matplotlib.pyplot as plt, heapq\n",
        "from datetime import datetime, timedelta\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import gymnasium as gym\n",
        "import yfinance as yf\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "from gym_anytrading.envs import StocksEnv\n",
        "from gymnasium.spaces import Box as GBox\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"gymnasium\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"jupyter_client.session\")\n",
        "warnings.filterwarnings(\"ignore\", message=\".*Gym has been unmaintained.*\")\n",
        "\n",
        "# IMPORTANT: reuse the compute_enhanced_features already defined above if present\n",
        "try:\n",
        "    compute_enhanced_features\n",
        "except NameError:\n",
        "    # FINAL fallback only if it truly doesn't exist\n",
        "    def compute_enhanced_features(df_in: pd.DataFrame) -> pd.DataFrame:\n",
        "        return df_in\n",
        "\n",
        "set_random_seed(42)\n",
        "\n",
        "\n",
        "\n",
        "# Configuration N_TAG = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "RESULTS_DIR = f\"/content/drive/MyDrive/Results_May_2025/ppo_walkforward_results_{RUN_TAG}\"\n",
        "\n",
        "\n",
        "def get_mu_sigma(model, obs):\n",
        "    \"\"\"SB3 v2-safe way to get Gaussian policy mean/std for continuous actions.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        obs_t, _ = model.policy.obs_to_tensor(obs)\n",
        "        features = model.policy.extract_features(obs_t)\n",
        "        latent_pi, _ = model.policy.mlp_extractor(features)\n",
        "        mean_actions = model.policy.action_net(latent_pi)\n",
        "        log_std = model.policy.log_std\n",
        "        mu = float(mean_actions.detach().cpu().numpy().squeeze())\n",
        "        sigma = float(log_std.exp().detach().cpu().numpy().squeeze())\n",
        "    return mu, sigma\n",
        "\n",
        "FINAL_MODEL_DIR = os.path.join(\"/content/drive/MyDrive/Results_May_2025\", \"ppo_models_master\")\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "os.makedirs(FINAL_MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# Global skip aggregation (thread-safe)\n",
        "from threading import Lock\n",
        "SKIP_AGG_PATH = os.path.join(RESULTS_DIR, \"skipped_windows_global.csv\")\n",
        "SKIP_LOCK = Lock()\n",
        "\n",
        "def record_skips_global(ticker: str, skipped_windows: list, total_windows: int = None, fully_skipped: bool = False):\n",
        "    \"\"\"\n",
        "    Append skipped windows to a global CSV so we can produce a recap at the end.\n",
        "    Windows should be like 'AAPL_window3'. If fully_skipped is True, all windows were pre-existing.\n",
        "    \"\"\"\n",
        "    if not skipped_windows and not fully_skipped:\n",
        "        return\n",
        "    import csv\n",
        "    with SKIP_LOCK:\n",
        "        new_file = not os.path.exists(SKIP_AGG_PATH)\n",
        "        with open(SKIP_AGG_PATH, \"a\", newline=\"\") as f:\n",
        "            w = csv.writer(f)\n",
        "            if new_file:\n",
        "                w.writerow([\"Ticker\", \"Window\", \"FullySkipped\", \"TotalWindows\"])\n",
        "            if fully_skipped:\n",
        "                # Record a single line indicating the ticker was completely skipped\n",
        "                w.writerow([ticker, \"ALL\", True, total_windows if total_windows is not None else \"\"])\n",
        "            else:\n",
        "                for wname in skipped_windows:\n",
        "                    # wname format expected: SYMBOL_windowX\n",
        "                    try:\n",
        "                        _, win_str = wname.split(\"_window\")\n",
        "                        win = int(win_str)\n",
        "                    except Exception:\n",
        "                        win = \"\"\n",
        "                    w.writerow([ticker, win, False, total_windows if total_windows is not None else \"\"])\n",
        "\n",
        "# Logging Setup\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    force=True\n",
        ")\n",
        "\n",
        "\n",
        "# Flags\n",
        "ENABLE_SENTIMENT = False\n",
        "ENABLE_SLO       = True\n",
        "ENABLE_WAVELET   = True\n",
        "test_mode        = False\n",
        "ENABLE_PLOTS     = False\n",
        "LIVE_MODE        = False           # Simple live/paper inference loop (set True to run)\n",
        "SIM_LATENCY_MS   = 0             # Network/broker latency simulation; 0 = off\n",
        "BROKER           = \"log\"           #\"log\" = do not place orders, just log\n",
        "\n",
        "# Global training settings\n",
        "WINDOW_SIZE = 3500\n",
        "STEP_SIZE = 500\n",
        "TIMESTEPS = 150_000\n",
        "\n",
        "# Load Dataset\n",
        "DATA_PATH = \"multi_stock_feature_engineered_dataset.csv\"\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    raise FileNotFoundError(\"Required feature-engineered dataset not found!\")\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
        "\n",
        "# NEW: ContinuousPositionEnv (continuous exposure + reward shaping)\n",
        "class ContinuousPositionEnv(StocksEnv):\n",
        "    def __init__(self, df, frame_bound, window_size,\n",
        "                 cost_rate=0.0002, slip_rate=0.0003,\n",
        "                 k_alpha=0.20, k_mom=0.05, k_sent=0.0,\n",
        "                 mom_source=\"denoised\", mom_lookback=20,\n",
        "                 min_trade_delta=0.01, cooldown=5, reward_clip=1.0):\n",
        "        super().__init__(df=df.reset_index(drop=True), frame_bound=frame_bound, window_size=window_size)\n",
        "        if isinstance(self.observation_space, gym.spaces.Box):\n",
        "            self.observation_space = GBox(\n",
        "                low=self.observation_space.low,\n",
        "                high=self.observation_space.high,\n",
        "                shape=self.observation_space.shape,\n",
        "                dtype=self.observation_space.dtype,\n",
        "            )\n",
        "\n",
        "        self.action_space = GBox(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)\n",
        "        self.cost_rate, self.slip_rate = float(cost_rate), float(slip_rate)\n",
        "        self.k_alpha, self.k_mom = float(k_alpha), float(k_mom)\n",
        "        self.k_sent = float(k_sent)\n",
        "        self.mom_source, self.mom_lookback = str(mom_source), int(mom_lookback)\n",
        "        self.min_trade_delta, self.cooldown = float(min_trade_delta), int(cooldown)\n",
        "        self.reward_clip = float(reward_clip)\n",
        "        self.nav, self.pos, self._last_trade_step = 1.0, 0.0, -self.cooldown\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        out = super().reset(**kwargs)\n",
        "        if isinstance(out, tuple):\n",
        "            obs, info = out\n",
        "        else:\n",
        "            obs, info = out, {}\n",
        "        self.nav, self.pos, self._last_trade_step = 1.0, 0.0, -self.cooldown\n",
        "        info = info or {}\n",
        "        info.update({\"nav\": self.nav, \"pos\": self.pos})\n",
        "        return obs, info\n",
        "\n",
        "    def _step_parent_hold(self):\n",
        "        step_result = super().step(2)\n",
        "        if len(step_result) == 5:\n",
        "            obs, _env_rew, terminated, truncated, info = step_result\n",
        "        else:\n",
        "            obs, _env_rew, done, info = step_result\n",
        "            terminated, truncated = bool(done), False\n",
        "        return obs, terminated, truncated, info\n",
        "\n",
        "    def _ret_t(self):\n",
        "        cur  = float(self.df.loc[self._current_tick, 'Close'])\n",
        "        prev = float(self.df.loc[max(self._current_tick - 1, 0), 'Close'])\n",
        "        return 0.0 if prev <= 0 else (cur - prev) / prev\n",
        "    def _mom_signal(self):\n",
        "        if self.mom_source == \"macd\" and \"MACD_Line\" in self.df.columns:\n",
        "            recent = self.df[\"MACD_Line\"].iloc[max(self._current_tick-200,0):self._current_tick+1]\n",
        "            return float(np.tanh(float(self.df.loc[self._current_tick, \"MACD_Line\"]) / (1e-6 + recent.std())))\n",
        "        if \"Denoised_Close\" in self.df.columns and self._current_tick - self.mom_lookback >= 0:\n",
        "            now  = float(self.df.loc[self._current_tick, \"Denoised_Close\"])\n",
        "            then = float(self.df.loc[self._current_tick - self.mom_lookback, \"Denoised_Close\"])\n",
        "            base = float(self.df.loc[max(self._current_tick - 1, 0), \"Close\"])\n",
        "            slope = (now - then) / max(self.mom_lookback, 1)\n",
        "            return float(np.tanh(10.0 * (slope / max(abs(base), 1e-6))))\n",
        "        return 0.0\n",
        "\n",
        "    def step(self, action):\n",
        "        try:\n",
        "            a = float(np.array(action).squeeze())\n",
        "            target_pos = float(np.clip(a, -1.0, 1.0))\n",
        "\n",
        "            r_t = self._ret_t()\n",
        "            base_ret = self.pos * r_t\n",
        "\n",
        "            changed = (abs(target_pos - self.pos) >= self.min_trade_delta) and \\\n",
        "                      ((self._current_tick - self._last_trade_step) >= self.cooldown)\n",
        "            delta_pos = (target_pos - self.pos) if changed else 0.0\n",
        "            trade_cost = (self.cost_rate + self.slip_rate) * abs(delta_pos)\n",
        "            rel_alpha = base_ret - r_t                    # Outperform b&h\n",
        "            mom_term  = self.pos * self._mom_signal()     # carry on trend\n",
        "            # Optional sentiment shaping\n",
        "            sent_term = 0.0\n",
        "            if ENABLE_SENTIMENT and \"SentimentScore\" in self.df.columns:\n",
        "                sent_term = self.k_sent * float(self.df.loc[self._current_tick, \"SentimentScore\"])\n",
        "            shaped = base_ret + self.k_alpha*rel_alpha + self.k_mom*mom_term + sent_term - trade_cost\n",
        "            reward = float(np.clip(shaped, -self.reward_clip, self.reward_clip))\n",
        "            self.nav *= (1.0 + base_ret - trade_cost)\n",
        "            if changed:\n",
        "                self.pos = target_pos\n",
        "                self._last_trade_step = self._current_tick\n",
        "            obs, terminated, truncated, info = self._step_parent_hold()\n",
        "            info = info or {}\n",
        "            info.update({\n",
        "                \"ret_t\": r_t, \"nav\": self.nav, \"pos\": self.pos,\n",
        "                \"trade_cost\": trade_cost, \"base_ret\": base_ret,\n",
        "                \"rel_alpha\": rel_alpha, \"mom\": self._mom_signal()\n",
        "            })\n",
        "            return obs, reward, terminated, truncated, info\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Step error: {e}\")\n",
        "            try:\n",
        "                obs, _ = self.reset()\n",
        "            except Exception:\n",
        "                obs = super().reset() if hasattr(super(), \"reset\") else None\n",
        "            return obs, 0.0, True, False, {}\n",
        "\n",
        "# Walkforward windowing\n",
        "def get_walk_forward_windows(df, window_size=3500, step_size=500, min_len=1200):\n",
        "    return [\n",
        "        (start, start + window_size)\n",
        "        for start in range(0, len(df) - min_len, step_size)\n",
        "        if start + window_size < len(df)\n",
        "    ]\n",
        "# Save QuantConnect-Compatible Artifacts\n",
        "def save_quantconnect_model(artifact, prefix, save_dir):\n",
        "    model_obj = artifact.get(\"model\", None)\n",
        "    model_path = os.path.join(save_dir, f\"{prefix}_model.zip\")\n",
        "    if model_obj is not None and not os.path.exists(model_path):\n",
        "        model_obj.save(model_path)\n",
        "\n",
        "    vecnorm_src = artifact.get(\"vecnorm_path\")\n",
        "    if vecnorm_src:\n",
        "        try:\n",
        "            vecnorm_dst = os.path.join(save_dir, f\"{prefix}_vecnorm.pkl\")\n",
        "            if os.path.abspath(vecnorm_src) != os.path.abspath(vecnorm_dst):\n",
        "                import shutil\n",
        "                shutil.copyfile(vecnorm_src, vecnorm_dst)\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"VecNormalize file handling issue for {prefix}: {e}\")\n",
        "\n",
        "    with open(os.path.join(save_dir, f\"{prefix}_features.json\"), \"w\") as f:\n",
        "        json.dump({\"features\": artifact['features']}, f)\n",
        "\n",
        "    with open(os.path.join(save_dir, f\"{prefix}_probability_config.json\"), \"w\") as f:\n",
        "        json.dump({\"threshold\": 0.05, \"use_confidence\": True, \"inference_mode\": \"deterministic\"}, f)\n",
        "\n",
        "    with open(os.path.join(save_dir, f\"{prefix}_model_info.json\"), \"w\") as f:\n",
        "        json.dump({\n",
        "            \"model\": \"PPO\",\n",
        "            \"ticker\": artifact['result']['Ticker'],\n",
        "            \"window\": artifact['result']['Window'],\n",
        "            \"date_trained\": datetime.today().strftime(\"%Y-%m-%d\"),\n",
        "            \"framework\": \"stable-baselines3\",\n",
        "            \"input_features\": artifact['features'],\n",
        "            \"final_portfolio\": artifact['result']['PPO_Portfolio'],\n",
        "            \"buy_hold\": artifact['result']['BuyHold'],\n",
        "            \"sharpe\": artifact['result']['Sharpe']\n",
        "        }, f)\n",
        "\n",
        "    logging.info(f\"Saved QuantConnect-compatible model for {artifact['result']['Ticker']} | {artifact['result']['Window']}\")\n",
        "\n",
        "# Live inference helpers (uses saved PPO + VecNormalize)\n",
        "def load_model_and_env(prefix):\n",
        "    \"\"\"Load a trained PPO and create a factory to build a matching env window.\"\"\"\n",
        "    model_path = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_model.zip\")\n",
        "    vec_path   = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_vecnorm.pkl\")\n",
        "    model = PPO.load(model_path, device=\"cpu\")\n",
        "\n",
        "    def make_env(df_window):\n",
        "        frame_bound = (50, len(df_window) - 3)\n",
        "        e = DummyVecEnv([lambda: ContinuousPositionEnv(\n",
        "            df=df_window, frame_bound=frame_bound, window_size=10,\n",
        "            cost_rate=(0.0002 if ENABLE_SLO else 0.0),\n",
        "            slip_rate=(0.0003 if ENABLE_SLO else 0.0),\n",
        "            k_alpha=0.20, k_mom=0.05, k_sent=(0.01 if ENABLE_SENTIMENT else 0.0),\n",
        "            mom_source=\"denoised\", mom_lookback=20,\n",
        "            min_trade_delta=0.01, cooldown=5, reward_clip=1.0\n",
        "        )])\n",
        "\n",
        "        if os.path.exists(vec_path):\n",
        "            e = VecNormalize.load(vec_path, e)\n",
        "        e.training = False\n",
        "        e.norm_reward = False\n",
        "        return e\n",
        "    return model, make_env\n",
        "\n",
        "def latest_df_for_symbol(symbol, horizon_days=5, interval=\"1m\"):\n",
        "    \"\"\"Fetch fresh bars and rebuild features exactly like training.\"\"\"\n",
        "    end = datetime.utcnow()\n",
        "    start = end - timedelta(days=horizon_days)\n",
        "    df = yf.download(\n",
        "        symbol,\n",
        "        start=start.strftime(\"%Y-%m-%d\"),\n",
        "        end=end.strftime(\"%Y-%m-%d\"),\n",
        "        interval=interval,\n",
        "        progress=False,\n",
        "        auto_adjust=False\n",
        "    )\n",
        "    if df is None or df.empty:\n",
        "        return None\n",
        "    df = df.reset_index()\n",
        "    df['Symbol'] = symbol\n",
        "    # IMPORTANT: reuse your same feature function\n",
        "    return compute_enhanced_features(df)\n",
        "\n",
        "def predict_latest(symbol, prefix):\n",
        "    \"\"\"Build last window, fast-forward env, call model.predict(), return a signal.\"\"\"\n",
        "    model, make_env = load_model_and_env(prefix)\n",
        "    live_df = latest_df_for_symbol(symbol)\n",
        "    if live_df is None or len(live_df) < 100:\n",
        "        logging.warning(\"No fresh data yet for live inference.\")\n",
        "        return None\n",
        "    df_window = live_df.iloc[-2500:].reset_index(drop=True) if len(live_df) > 2500 else live_df.copy()\n",
        "    env = make_env(df_window)\n",
        "    obs = env.reset()\n",
        "    if isinstance(obs, tuple):\n",
        "        obs, _ = obs\n",
        "    for _ in range(len(df_window) - 1):\n",
        "        obs, _, dones, _ = env.step([np.array([0.0], dtype=np.float32)])\n",
        "        if isinstance(dones, (np.ndarray, list)) and dones[0]:\n",
        "            break\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    mu, sigma = get_mu_sigma(model, obs)\n",
        "    from math import erf, sqrt\n",
        "    def Phi(x):  # Standard normal CDF\n",
        "        return 0.5 * (1.0 + erf(x / sqrt(2.0)))\n",
        "    # Approximate P(a>0) using Gaussian in action space (pre-squash)\n",
        "    p_long  = 1.0 - Phi((0.0 - mu) / max(sigma, 1e-6))\n",
        "    p_short = 1.0 - p_long\n",
        "    a = float(np.array(action).squeeze())\n",
        "    signal = \"BUY\" if a > 0.1 else (\"SELL\" if a < -0.3 else \"HOLD\")\n",
        "    conf = abs(a)\n",
        "    ts = df_window['Datetime'].iloc[-1] if 'Datetime' in df_window.columns else None\n",
        "    price = float(df_window['Close'].iloc[-1])\n",
        "    return dict(signal=signal, confidence=conf, action=a, ts=ts, price=price,\n",
        "                p_long=p_long, p_short=p_short, mu=mu, sigma=sigma)\n",
        "def place_order(signal, qty=1):\n",
        "    \"\"\"Stub broker router with latency simulation; logs in Colab.\"\"\"\n",
        "    if SIM_LATENCY_MS > 0:\n",
        "        time.sleep(SIM_LATENCY_MS / 1000.0)\n",
        "    if BROKER == \"log\":\n",
        "        logging.info(f\"[PAPER] {signal} x{qty}\")\n",
        "    else:\n",
        "        # Hook up real broker SDK here (Alpaca, IB, etc.)\n",
        "        logging.info(f\"[BROKER={BROKER}] {signal} x{qty} (not implemented)\")\n",
        "\n",
        "def live_loop(symbol, best_prefix):\n",
        "    \"\"\"Simple polling loopâ€”set LIVE_MODE=True to run.\"\"\"\n",
        "    while LIVE_MODE:\n",
        "        try:\n",
        "            pred = predict_latest(symbol, best_prefix)\n",
        "            if pred:\n",
        "                logging.info(\n",
        "                    f\"{symbol} {pred['ts']} | {pred['signal']} @ {pred['price']:.2f} (conf {pred['confidence']:.2f})\"\n",
        "                )\n",
        "                place_order(pred['signal'], qty=1)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Live loop error: {e}\")\n",
        "        time.sleep(60)  # Poll each minute\n",
        "TOP_N_WINDOWS = 3\n",
        "# PPO Walkforward Function\n",
        "def walkforward_ppo(df, ticker, window_size=3500, step_size=500,\n",
        "                    timesteps=150_000, learning_rate=1e-4, ppo_overrides=None):\n",
        "    if ppo_overrides is None:\n",
        "        ppo_overrides = {}\n",
        "    if len(df) < window_size:\n",
        "        logging.warning(f\"Skipping {ticker}: only {len(df)} rows (min required: {window_size})\")\n",
        "        return []\n",
        "    results = []\n",
        "    windows = get_walk_forward_windows(df, window_size, step_size)\n",
        "    top_heap = []\n",
        "    skipped_windows = []\n",
        "    all_done = True\n",
        "    for idx in range(len(windows)):\n",
        "        prefix = f\"ppo_{ticker}_window{idx+1}\"\n",
        "        model_ok   = os.path.exists(os.path.join(FINAL_MODEL_DIR, f\"{prefix}_model.zip\"))\n",
        "        vecnorm_ok = os.path.exists(os.path.join(FINAL_MODEL_DIR, f\"{prefix}_vecnorm.pkl\"))\n",
        "        if not (model_ok and vecnorm_ok):\n",
        "            all_done = False\n",
        "            break\n",
        "\n",
        "    if all_done:\n",
        "        logging.info(f\"Ticker {ticker} fully skipped (all {len(windows)} windows already complete).\")\n",
        "        record_skips_global(ticker, skipped_windows=[], total_windows=len(windows), fully_skipped=True)\n",
        "        return []\n",
        "    # Main window loop\n",
        "    for w_idx, (start, end) in enumerate(windows):\n",
        "        window_start_time = time.time()\n",
        "        gc.collect()\n",
        "        prefix = f\"ppo_{ticker}_window{w_idx+1}\"\n",
        "        model_path   = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_model.zip\")\n",
        "        vecnorm_path = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_vecnorm.pkl\")\n",
        "        if os.path.exists(model_path) and os.path.exists(vecnorm_path):\n",
        "            logging.info(f\"Skipping {ticker} | Window {w_idx+1}, already trained.\")\n",
        "            skipped_windows.append(f\"{ticker}_window{w_idx+1}\")\n",
        "            continue\n",
        "        missing = []\n",
        "        if not os.path.exists(model_path):   missing.append(\"model.zip\")\n",
        "        if not os.path.exists(vecnorm_path): missing.append(\"vecnorm.pkl\")\n",
        "        logging.info(f\"Will train {ticker} | Window {w_idx+1} because missing: {', '.join(missing)}\")\n",
        "        df_window = df.iloc[start:end].reset_index(drop=True)\n",
        "        if len(df_window) <= 52 or len(df_window) % 2 != 0:\n",
        "            df_window = df_window.iloc[:-1]\n",
        "        frame_bound = (50, len(df_window) - 3)\n",
        "        env = DummyVecEnv([lambda: ContinuousPositionEnv(\n",
        "            df=df_window, frame_bound=frame_bound, window_size=10,\n",
        "            cost_rate=0.0002, slip_rate=0.0003,\n",
        "            k_alpha=0.20, k_mom=0.05,\n",
        "            k_sent=(0.01 if ENABLE_SENTIMENT else 0.0),\n",
        "            mom_source=\"denoised\", mom_lookback=20,\n",
        "            min_trade_delta=0.01, cooldown=5, reward_clip=1.0\n",
        "        )])\n",
        "        env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)\n",
        "\n",
        "        try:\n",
        "            # Train\n",
        "            model = PPO(\n",
        "                \"MlpPolicy\",\n",
        "                env,\n",
        "                verbose=0,\n",
        "                device=(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "                learning_rate=ppo_overrides.get(\"lr\", learning_rate),\n",
        "                n_steps=ppo_overrides.get(\"n_steps\", 256),\n",
        "                batch_size=ppo_overrides.get(\"batch\", 64),\n",
        "                n_epochs=5,\n",
        "                gamma=0.99,\n",
        "                gae_lambda=0.95,\n",
        "                clip_range=ppo_overrides.get(\"clip\", 0.2),\n",
        "                ent_coef=ppo_overrides.get(\"ent\", 0.005),\n",
        "                policy_kwargs=dict(net_arch=[64, 64]),\n",
        "            )\n",
        "\n",
        "            logging.info(f\"Training {ticker} Window {w_idx+1}/{len(windows)}\")\n",
        "            model.learn(total_timesteps=timesteps)\n",
        "            env.training = False\n",
        "            env.norm_reward = False\n",
        "            obs = env.reset()\n",
        "            if isinstance(obs, tuple):\n",
        "                obs, _ = obs\n",
        "            nav_track, bh_track = [1.0], [1.0]\n",
        "            step_log = []  # Per-step rich logs\n",
        "            for i in range(len(df_window) - 1):\n",
        "                action, _ = model.predict(obs, deterministic=True)\n",
        "                mu, sigma = get_mu_sigma(model, obs)\n",
        "                obs, _, dones, infos = env.step(action)\n",
        "                info = infos[0] if isinstance(infos, (list, tuple)) else infos\n",
        "\n",
        "                nav_track.append(float(info.get(\"nav\", nav_track[-1])))\n",
        "                bh_track.append(bh_track[-1] * (1.0 + float(info.get(\"ret_t\", 0.0))))\n",
        "\n",
        "                a = float(np.array(action).squeeze())\n",
        "                dt_val = df_window[\"Datetime\"].iloc[i+1] if \"Datetime\" in df_window.columns else None\n",
        "                px     = float(df_window[\"Close\"].iloc[i+1]) if \"Close\" in df_window.columns else np.nan\n",
        "                step_log.append({\n",
        "                    \"Index\": i+1, \"Datetime\": dt_val, \"Close\": px, \"Action\": a,\n",
        "                    \"mu\": mu, \"sigma\": sigma, \"nav\": nav_track[-1],\n",
        "                    \"ret_t\": float(info.get(\"ret_t\", 0.0)),\n",
        "                    \"pos\": float(info.get(\"pos\", 0.0)),\n",
        "                    \"trade_cost\": float(info.get(\"trade_cost\", 0.0)),\n",
        "                    \"base_ret\": float(info.get(\"base_ret\", 0.0)),\n",
        "                    \"rel_alpha\": float(info.get(\"rel_alpha\", 0.0)),\n",
        "                    \"mom\": float(info.get(\"mom\", 0.0)),\n",
        "                })\n",
        "\n",
        "                if isinstance(dones, (np.ndarray, list, tuple)):\n",
        "                    if dones[0]:\n",
        "                        break\n",
        "                elif dones:\n",
        "                    break\n",
        "            # Metrics\n",
        "            final_value = float(nav_track[-1]) * 100000.0\n",
        "            hold_value  = float(bh_track[-1])  * 100000.0\n",
        "            returns = pd.Series(nav_track).pct_change().fillna(0.0)\n",
        "            sharpe  = float((returns.mean() / (returns.std() + 1e-9)) * np.sqrt(252))\n",
        "            drawdown = float(((pd.Series(nav_track).cummax() - pd.Series(nav_track)) /\n",
        "                              pd.Series(nav_track).cummax()).max() * 100)\n",
        "\n",
        "            # Save model + VecNormalize\n",
        "            vecnorm_path = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_vecnorm.pkl\")\n",
        "            try:\n",
        "                env.save(vecnorm_path)\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"Could not save VecNormalize for {ticker} {start}-{end}: {e}\")\n",
        "                vecnorm_path = None\n",
        "\n",
        "            model_path = os.path.join(FINAL_MODEL_DIR, f\"{prefix}_model.zip\")\n",
        "            model.save(model_path)\n",
        "\n",
        "            # Save predictions (both schemas)\n",
        "            pred_path = os.path.join(RESULTS_DIR, f\"{prefix}_predictions.csv\")\n",
        "            pd.DataFrame(step_log).to_csv(pred_path, index=False)\n",
        "            logging.info(f\"Saved predictions to {pred_path}\")\n",
        "\n",
        "            compat_rows = []\n",
        "            for r in step_log:\n",
        "                a = r[\"Action\"]\n",
        "                signal = \"BUY\" if a > 0.1 else (\"SELL\" if a < -0.3 else \"HOLD\")\n",
        "                compat_rows.append({\n",
        "                    \"Index\": r[\"Index\"], \"Datetime\": r[\"Datetime\"], \"Close\": r[\"Close\"],\n",
        "                    \"Action\": a, \"Signal\": signal, \"PortfolioValue\": r[\"nav\"], \"Reward\": np.nan,\n",
        "                })\n",
        "            compat_path = os.path.join(RESULTS_DIR, f\"{prefix}_predictions_compat.csv\")\n",
        "            pd.DataFrame(compat_rows).to_csv(compat_path, index=False)\n",
        "            logging.info(f\"Saved compatibility predictions to {compat_path}\")\n",
        "\n",
        "            # Summary row\n",
        "            result_row = {\n",
        "                \"Ticker\": ticker,\n",
        "                \"Window\": f\"{start}-{end}\",\n",
        "                \"PPO_Portfolio\": round(final_value, 2),\n",
        "                \"BuyHold\": round(hold_value, 2),\n",
        "                \"Sharpe\": round(sharpe, 3),\n",
        "                \"Drawdown_%\": round(drawdown, 2),\n",
        "                \"Winner\": \"PPO\" if final_value > hold_value else \"Buy & Hold\",\n",
        "            }\n",
        "            results.append(result_row)\n",
        "\n",
        "            meta = {\n",
        "                \"result\": result_row,\n",
        "                \"features\": df_window.columns.tolist(),\n",
        "                \"prefix\": prefix,\n",
        "                \"model_path\": model_path,\n",
        "                \"vecnorm_path\": vecnorm_path,\n",
        "            }\n",
        "\n",
        "            item = (result_row[\"Sharpe\"], prefix, meta)\n",
        "            if len(top_heap) < TOP_N_WINDOWS:\n",
        "                heapq.heappush(top_heap, item)\n",
        "            else:\n",
        "                if item[0] > top_heap[0][0]:\n",
        "                    heapq.heapreplace(top_heap, item)\n",
        "            logging.info(f\"{ticker} | Window {w_idx+1} runtime: {round(time.time()-window_start_time, 2)}s\")\n",
        "        finally:\n",
        "            try: env.close()\n",
        "            except Exception: pass\n",
        "            del env\n",
        "            try: del model\n",
        "            except Exception: pass\n",
        "            gc.collect()\n",
        "            try: torch.cuda.empty_cache()\n",
        "            except Exception: pass\n",
        "    if skipped_windows:\n",
        "        logging.info(f\"{ticker} skipped windows (already complete): {', '.join(skipped_windows)}\")\n",
        "        record_skips_global(ticker, skipped_windows=skipped_windows, total_windows=len(windows), fully_skipped=False)\n",
        "\n",
        "\n",
        "    # Save top-N (artifact wrapper reads from paths)\n",
        "    top_list = sorted(top_heap, key=lambda t: t[0], reverse=True)\n",
        "    for _, _, meta in top_list:\n",
        "        artifact_for_save = {\n",
        "            \"model\": None,\n",
        "            \"vecnorm_path\": meta[\"vecnorm_path\"],\n",
        "            \"features\": meta[\"features\"],\n",
        "            \"result\": meta[\"result\"],\n",
        "            \"prefix\": meta[\"prefix\"]\n",
        "        }\n",
        "        save_quantconnect_model(artifact_for_save, meta[\"prefix\"], FINAL_MODEL_DIR)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Per-bucket PPO configs\n",
        "FAST = {\"lr\": 5e-5,  \"n_steps\": 1024, \"batch\": 128, \"clip\": 0.25, \"ent\": 0.015}\n",
        "SLOW = {\"lr\": 1.5e-5,\"n_steps\": 2048, \"batch\": 64,  \"clip\": 0.16, \"ent\": 0.0075}\n",
        "\n",
        "fast_names = {\n",
        "    \"TSLA\",\"NVDA\",\"AMD\",\"AVGO\",\"AAPL\",\"MSFT\",\"AMZN\",\"GOOGL\",\"META\",\"ADBE\",\"CRM\",\n",
        "    \"INTC\",\"QCOM\",\"TXN\",\"ORCL\",\"NEE\",\"GE\",\"XOM\",\"CVX\",\"LLY\",\"NKE\",\"SBUX\"\n",
        "}\n",
        "slow_names = {\n",
        "    \"BRK-B\",\"JPM\",\"BAC\",\"JNJ\",\"UNH\",\"MRK\",\"PFE\",\"ABBV\",\"ABT\",\"AMGN\",\"PG\",\"PEP\",\"KO\",\n",
        "    \"V\",\"MA\",\"WMT\",\"MCD\",\"TMO\",\"DHR\",\"ACN\",\"IBM\",\"LIN\",\"PM\",\"RTX\",\"UPS\",\"UNP\",\"COST\",\"HD\",\"LOW\"\n",
        "}\n",
        "\n",
        "def pick_params(symbol: str):\n",
        "    return FAST if symbol in fast_names else SLOW\n",
        "\n",
        "def process_ticker(ticker):\n",
        "    try:\n",
        "        hp = pick_params(ticker)\n",
        "        return walkforward_ppo(\n",
        "            df[df['Symbol'] == ticker].copy(),\n",
        "            ticker,\n",
        "            window_size=WINDOW_SIZE,\n",
        "            timesteps=TIMESTEPS,\n",
        "            learning_rate=hp[\"lr\"],\n",
        "            ppo_overrides=hp\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logging.error(f\"{ticker}: training failed with {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "\n",
        "def run_parallel_tickers(tickers, out_path=os.path.join(RESULTS_DIR, \"summary.csv\"), max_workers=8):\n",
        "    results = []\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
        "        for res in ex.map(process_ticker, tickers):\n",
        "            if res:\n",
        "                results.extend(res)\n",
        "                pd.DataFrame(results).to_csv(out_path, index=False)\n",
        "    logging.info(\"All tickers processed.\")\n",
        "    return results\n",
        "\n",
        "# Execution Block\n",
        "if __name__ == \"__main__\":\n",
        "    min_rows = WINDOW_SIZE + 50  # Use global constant\n",
        "    symbols = []\n",
        "    for s, n in df['Symbol'].value_counts().items():\n",
        "        if n >= min_rows:\n",
        "            symbols.append(s)\n",
        "        else:\n",
        "            logging.warning(f\"Skipping {s}: only {n} rows (< {min_rows} required)\")\n",
        "    need_cols = [\"Close\", \"Datetime\"]\n",
        "    if ENABLE_WAVELET:\n",
        "        need_cols.append(\"Denoised_Close\")\n",
        "    if ENABLE_SENTIMENT:\n",
        "        need_cols.append(\"SentimentScore\")\n",
        "\n",
        "    valid_symbols = []\n",
        "    for s in symbols:\n",
        "        cols = set(df.loc[df.Symbol == s].columns)\n",
        "        missing = [c for c in need_cols if c not in cols]\n",
        "        if missing:\n",
        "            logging.warning(f\"Skipping {s}: missing {missing}\")\n",
        "        else:\n",
        "            valid_symbols.append(s)\n",
        "    CONFIG = {'symbols': valid_symbols}\n",
        "    all_results = []\n",
        "    if test_mode:\n",
        "        test_stocks = ['AAPL', 'NVDA', 'MSFT']\n",
        "        for stock in test_stocks:\n",
        "            logging.info(f\">>> Running test_mode on {stock}\")\n",
        "            results = process_ticker(stock)\n",
        "            all_results.extend(results)\n",
        "\n",
        "        test_summary_path = os.path.join(RESULTS_DIR, \"summary_test_mode.csv\")\n",
        "        pd.DataFrame(all_results).to_csv(test_summary_path, index=False)\n",
        "        logging.info(f\"Test summary saved to {test_summary_path}\")\n",
        "    else:\n",
        "        summary_results = run_parallel_tickers(CONFIG['symbols'])\n",
        "        if not summary_results:\n",
        "            logging.warning(\"No results generated.\")\n",
        "        else:\n",
        "            path = os.path.join(RESULTS_DIR, \"summary.csv\")\n",
        "            pd.DataFrame(summary_results).to_csv(path, index=False)\n",
        "            logging.info(f\"Summary saved to {path}\")\n",
        "    try:\n",
        "        if os.path.exists(SKIP_AGG_PATH):\n",
        "            recap = pd.read_csv(SKIP_AGG_PATH)\n",
        "            fs = recap[\"FullySkipped\"].astype(str).str.lower().isin([\"true\", \"1\", \"yes\"])\n",
        "            recap[\"FullySkipped\"] = fs\n",
        "            fully = recap[recap[\"FullySkipped\"]][\"Ticker\"].dropna().unique().tolist()\n",
        "            if fully:\n",
        "                logging.info(f\"Fully skipped tickers (all windows done): {', '.join(fully)}\")\n",
        "            part = recap[~recap[\"FullySkipped\"]]\n",
        "            if not part.empty:\n",
        "                counts = part.groupby(\"Ticker\")[\"Window\"].count().sort_values(ascending=False)\n",
        "                logging.info(\"Partially skipped window counts per ticker:\")\n",
        "                for t, c in counts.items():\n",
        "                    logging.info(f\"   - {t}: {c} window(s) already complete\")\n",
        "            logging.info(f\"Global skip log: {SKIP_AGG_PATH}\")\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Could not produce global skip recap: {e}\")\n",
        "\n",
        "\n",
        "    best_prefix = None\n",
        "    if LIVE_MODE and best_prefix is not None:\n",
        "        live_loop(\"AAPL\", best_prefix)d"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, logging\n",
        "from google.colab import drive\n",
        "\n",
        "#1) Make sure Drive is mounted\n",
        "if not os.path.ismount(\"/content/drive\"):\n",
        "    drive.mount(\"/content/drive\")\n",
        "\n",
        "FINAL_MODEL_DIR = \"/content/drive/MyDrive/Results_May_2025/ppo_models_master\"\n",
        "\n",
        "#2) Sanity: how many artifacts do we have?\n",
        "zips  = glob.glob(os.path.join(FINAL_MODEL_DIR, \"*.zip\"))\n",
        "pkl   = glob.glob(os.path.join(FINAL_MODEL_DIR, \"*.pkl\"))\n",
        "print(f\"Master folder: {FINAL_MODEL_DIR}\")\n",
        "print(f\"Found {len(zips)} model.zip and {len(pkl)} vecnorm.pkl\")\n",
        "\n",
        "#3) Show a few missing pairs for known tickers/windows (non-destructive)\n",
        "def check_prefix(prefix):\n",
        "    m = os.path.exists(os.path.join(FINAL_MODEL_DIR, f\"{prefix}_model.zip\"))\n",
        "    v = os.path.exists(os.path.join(FINAL_MODEL_DIR, f\"{prefix}_vecnorm.pkl\"))\n",
        "    return m, v\n",
        "\n",
        "sample = [\"ppo_AAPL_window1\",\"ppo_QCOM_window1\",\"ppo_BRK-B_window1\",\"ppo_SBUX_window1\"]\n",
        "for pref in sample:\n",
        "    m,v = check_prefix(pref)\n",
        "    print(f\"{pref}: model={'OK' if m else 'MISSING'}, vecnorm={'OK' if v else 'MISSING'}\")\n"
      ],
      "metadata": {
        "id": "eczZ9e06pNcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "all_summaries = []\n",
        "for p in glob.glob(os.path.join(BASE, \"ppo_walkforward_results_*\", \"summary*.csv\")):\n",
        "    try:\n",
        "        tmp = pd.read_csv(p)\n",
        "        tmp[\"RunFolder\"] = os.path.dirname(p)\n",
        "        all_summaries.append(tmp)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "if all_summaries:\n",
        "    combo = pd.concat(all_summaries, ignore_index=True)\n",
        "    combo = combo.drop_duplicates(subset=[\"Ticker\",\"Window\"], keep=\"last\")\n",
        "    out = os.path.join(FINAL, \"all_runs_summary.csv\")\n",
        "    combo.to_csv(out, index=False)\n",
        "    print(\" Saved:\", out, \"rows:\", len(combo))\n"
      ],
      "metadata": {
        "id": "6cMq8kbmgzts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Align live features to training features\n",
        "import os, json, numpy as np, pandas as pd\n",
        "from datetime import datetime, timedelta, timezone\n",
        "\n",
        "def align_to_training_features(df: pd.DataFrame, prefix: str, master_dir=FINAL_MODEL_DIR) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Make df match the exact feature set & order used at training for this prefix.\n",
        "    Adds missing columns with sensible defaults and reorders columns.\n",
        "    \"\"\"\n",
        "    feats_path = os.path.join(master_dir, f\"{prefix}_features.json\")\n",
        "    if not os.path.exists(feats_path):\n",
        "        return df\n",
        "    feat_list = json.load(open(feats_path))[\"features\"]\n",
        "    present = set(df.columns)\n",
        "    need = [c for c in feat_list if c not in present]\n",
        "    if \"Return\" in need and \"Close\" in df.columns:\n",
        "        df[\"Return\"] = df[\"Close\"].pct_change().fillna(0.0).astype(float)\n",
        "        present.add(\"Return\")\n",
        "    if \"Target\" in need:\n",
        "        if \"Return\" in df.columns:\n",
        "            df[\"Target\"] = np.sign(df[\"Return\"].shift(-1)).fillna(0.0).astype(float)\n",
        "        else:\n",
        "            df[\"Target\"] = 0.0\n",
        "        present.add(\"Target\")\n",
        "    for c in need:\n",
        "        if c not in df.columns:\n",
        "            df[c] = 0.0\n",
        "    return df[feat_list]\n",
        "\n",
        "# Replace utcnow() with timezone-aware now()\n",
        "def latest_df_for_symbol(symbol, horizon_days=5, interval=\"1m\"):\n",
        "    end = datetime.now(timezone.utc)\n",
        "    start = end - timedelta(days=horizon_days)\n",
        "    df = yf.download(\n",
        "        symbol,\n",
        "        start=start.strftime(\"%Y-%m-%d\"),\n",
        "        end=end.strftime(\"%Y-%m-%d\"),\n",
        "        interval=interval,\n",
        "        progress=False,\n",
        "        auto_adjust=False\n",
        "    )\n",
        "    if df is None or df.empty:\n",
        "        return None\n",
        "    df = df.reset_index()\n",
        "    df[\"Symbol\"] = symbol\n",
        "    return compute_enhanced_features(df)\n",
        "\n",
        "# Ensure predict_latest aligns to training features before building the env\n",
        "def predict_latest(symbol, prefix):\n",
        "    model, make_env = load_model_and_env(prefix)\n",
        "    live_df = latest_df_for_symbol(symbol)\n",
        "    if live_df is None or len(live_df) < 100:\n",
        "        return {\"error\": \"no fresh data\"}\n",
        "\n",
        "    # Align features to training for THIS prefix\n",
        "    live_df = align_to_training_features(live_df, prefix)\n",
        "\n",
        "    # Use the last ~2500 rows like you did before\n",
        "    df_window = live_df.iloc[-2500:].reset_index(drop=True) if len(live_df) > 2500 else live_df.copy()\n",
        "\n",
        "    env = make_env(df_window)\n",
        "    obs = env.reset()\n",
        "    if isinstance(obs, tuple):\n",
        "        obs, _ = obs\n",
        "\n",
        "    # Fast-forward with no-op action\n",
        "    for _ in range(len(df_window) - 1):\n",
        "        obs, _, dones, _ = env.step([np.array([0.0], dtype=np.float32)])\n",
        "        if isinstance(dones, (np.ndarray, list)) and dones[0]:\n",
        "            break\n",
        "\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "\n",
        "    # Diagnostics (unchanged)\n",
        "    mu, sigma = get_mu_sigma(model, obs)\n",
        "    from math import erf, sqrt\n",
        "    Phi = lambda x: 0.5 * (1.0 + erf(x / sqrt(2.0)))\n",
        "    p_long  = 1.0 - Phi((0.0 - mu) / max(sigma, 1e-6))\n",
        "    a = float(np.array(action).squeeze())\n",
        "    signal = \"BUY\" if a > 0.1 else (\"SELL\" if a < -0.3 else \"HOLD\")\n",
        "    ts = df_window[\"Datetime\"].iloc[-1] if \"Datetime\" in df_window.columns else None\n",
        "    price = float(df_window[\"Close\"].iloc[-1])\n",
        "    return dict(signal=signal, confidence=abs(a), action=a, ts=ts, price=price,\n",
        "                p_long=p_long, p_short=1.0-p_long, mu=mu, sigma=sigma)\n"
      ],
      "metadata": {
        "id": "HmmpUDAWEAX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json\n",
        "\n",
        "MASTER_DIR = \"/content/drive/MyDrive/Results_May_2025/ppo_models_master\"\n",
        "targets = [\"ppo_UNH_window3\", \"ppo_TSLA_window2\", \"ppo_TMO_window3\"]\n",
        "need = [\"_model.zip\",\"_vecnorm.pkl\",\"_features.json\",\"_probability_config.json\",\"_model_info.json\"]\n",
        "\n",
        "for p in targets:\n",
        "    status = {s: os.path.exists(os.path.join(MASTER_DIR, p+s)) for s in need}\n",
        "    print(p, status)\n",
        "    # Quick features sanity\n",
        "    fpath = os.path.join(MASTER_DIR, p+\"_features.json\")\n",
        "    if status[\"_features.json\"]:\n",
        "        feats = json.load(open(fpath)).get(\"features\", [])\n",
        "        print(f\"  features: {len(feats)} cols; first few: {feats[:8]}\")\n"
      ],
      "metadata": {
        "id": "f3QHqKKtUhRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = [(\"UNH\",\"ppo_UNH_window3\"), (\"TSLA\",\"ppo_TSLA_window2\"), (\"TMO\",\"ppo_TMO_window3\")]\n",
        "for sym, pref in pairs:\n",
        "    out = predict_latest(sym, pref)   # Downloads a small recent window and predicts once\n",
        "    print(sym, pref, \"->\", out)\n"
      ],
      "metadata": {
        "id": "TnbiaLe8UikM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
